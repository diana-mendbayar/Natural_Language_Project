{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Session 2 - Word representation",
   "metadata": {
    "tags": [],
    "cell_id": "00000-b8aa5e47-08f7-44a8-a9e9-6e9405f869ab",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Task 1\n\nGeneralize all the preprocessing tasks into one single function that can be use in the Vectorizer (passing boolean to make sure that you can activate or deactivate on preprocessing step)\n\nTask 2\n\nWork and research on Hashing Vectorizer. What advantage and disadvantage it gives. Implement it for you project (use TFidf for the rest)\n\n\nResearch\n\nResearch and create a presentation of the Latent Dirichlet Allocation (LDA) model.\n\nTask 4\n\nImplement your research model in order to predict industries on your dataset\n\nTask 5\n\nLearn and implement techniques to evaluate your model [Use sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html)\n\nTask 6 (Optional)\n\nCreate a WordCloud for each cluster predicted by your model",
   "metadata": {
    "tags": [],
    "cell_id": "00001-8d5024ce-04a1-4c62-915d-71b31788f5a8",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00002-40800ee5-349e-4d03-985f-702270482966",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "744d7329",
    "execution_start": 1635959737584,
    "execution_millis": 25254,
    "deepnote_cell_type": "code"
   },
   "source": "import pandas as pd\nimport random\nimport regex as re\nimport unicodedata\nimport nltk\nimport spacy\nimport string\nfrom sklearn.feature_extraction.text import HashingVectorizer\n!python -m spacy download en_core_web_sm >> /dev/null\n!pip install gensim",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "2021-11-03 17:15:50.033066: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2021-11-03 17:15:50.033102: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\nCollecting gensim\n  Downloading gensim-4.1.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n\u001b[K     |████████████████████████████████| 24.1 MB 27.5 MB/s \n\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from gensim) (1.7.1)\nRequirement already satisfied: smart-open>=1.8.1 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from gensim) (5.2.1)\nRequirement already satisfied: numpy>=1.17.0 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from gensim) (1.19.5)\nInstalling collected packages: gensim\nSuccessfully installed gensim-4.1.2\n\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00003-45b414e3-decd-4941-aa22-68905b17b49d",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e9c84f51",
    "execution_start": 1635959823266,
    "execution_millis": 1614,
    "deepnote_cell_type": "code"
   },
   "source": "dataset = pd.read_csv('employer_raw_data.csv')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00004-dc1c91b9-2055-4e7d-b912-74fd74b599cc",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e0c8d8ca",
    "execution_start": 1635959828622,
    "execution_millis": 2,
    "deepnote_cell_type": "code"
   },
   "source": "\nold_sentences = dataset['description'].values",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00002-3afe434b-6e66-48a2-a725-99c06dfdc57e",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d7b3f97a",
    "execution_start": 1635959838531,
    "execution_millis": 1,
    "deepnote_cell_type": "code"
   },
   "source": "# Preprocessing function\n\ndef get_preprocessing_function(\n    use_lower: bool = True,\n    use_alpha: bool = True,\n    use_stemming: bool = False,\n    use_lemmatization: bool = True,\n    punctuation: bool = True,\n    numbers: bool = True,\n    url: bool = True\n):\n\n    # Remove punctuation\n    STRING_PUNCTUATION = string.punctuation\n    def punctuation(text: str):\n        return text.translate(str.maketrans(\"\", \"\", STRING_PUNCTUATION)) if punctuation else text\n\n    # Remove numbers\n    def numbers(text: str):\n        return ''.join([i for i in text if not i.isdigit()]) if numbers else text\n\n    # Remove URLS\n    def urls(text: str):\n        url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n        return url_pattern.sub(r'', text) if url else text\n    \n    # Remove alpha numerics\n    def alpha(text: str):\n        return re.sub(\"[^a-z]+\", \" \", text) if use_alpha else text\n\n    # Make lowercase\n    def lower(text: str):\n        return text.lower() if use_lower else text\n    \n    # Implement stemming\n    def stemming(text: str):\n        stemmer = nltk.stem.PorterStemmer()\n        return \" \".join([stemmer.stem(word) for word in text.split(' ')]) if use_stemming else text\n    \n    \n    def preprocess(text: str):\n        #Order of processing steps\n        steps = [\n            lower, \n            punctuation, \n            numbers, \n            urls, \n            alpha, \n            stemming\n            ]\n        for step in steps:\n            text = step(text)\n        return text\n    \n    return preprocess",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00004-20edb9e6-7d0c-470f-998c-187acf8bc21d",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "9bd1547c",
    "execution_start": 1635959844143,
    "execution_millis": 24,
    "deepnote_cell_type": "code"
   },
   "source": "# Define preprocess function and apply to random sentence\n\npreprocess = get_preprocessing_function(\n    use_lower = True,\n    use_alpha = True,\n    use_stemming = True,\n    punctuation = True,\n    numbers = True,\n    url = True\n)\n\nsentence = random.choice(list(old_sentences))\nprocessed_sentence = preprocess(sentence)\n\nprint(f\"\"\"\nNon processed corpus:\n{sentence}\n------------------------\nProcessed corpus:\n{processed_sentence}\n\"\"\")",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "\nNon processed corpus:\nSTMicroelectronics is a global independent semiconductor company and a leader in developing and delivering semiconductor solutions across the spectrum of microelectronics applications. An unrivaled combination of silicon and system expertise, manufacturing strength, Intellectual Property (IP) portfolio, and strategic partners positions, STMicroelectronics is at the forefront of System-on-Chip ... STMicroelectronics Standard Products are a broad range of industry-standard and drop-in replacements for the most popular general-purpose analog ICs, discrete and serial EEPROMs. The Standard Products are manufactured to the highest quality standards with many AECQ-qualified for automotive applications. ... Mouser® and Mouser Electronics® are ... Cut Tape. Product is cut from a full reel tape into customized quantities. MouseReel™ (Add $7.00 reeling fee) A product reel is cut according to customer-specified quantities. All MouseReel orders are non-cancellable and non-returnable. Full Reels. The ordered quantity must match the manufacturer's full reel quantity. MouseReel™ (Add $7.00 reeling fee) A product reel is cut according to customer-specified quantities. All MouseReel orders are non-cancellable and non-returnable. Full Reels. The ordered quantity must match the manufacturer's full reel quantity. To purchase full reel, order in multiples of 4000. Reel and Cut Tape. STM32F103CBT6 STMicroelectronics ARM Microcontrollers - MCU 32BIT Cortex M3 128K MED Performance LN datasheet, inventory, & pricing. STMicroelectronics is a global independent semiconductor company and a leader in developing and delivering semiconductor solutions across the spectrum of microelectronics applications. An unrivaled combination of silicon and system expertise, manufacturing strength, Intellectual Property (IP) portfolio, and strategic partners positions, STMicroelectronics is at the forefront of System-on-Chip ... STMicroelectronics is a leading Integrated Device Manufacturer delivering solutions that are key to Smart Driving, Smart Industry, Smart Home & City and Smart Things. For quotes and prices in local currency, please select your country/region to view our local ST sales offices and distributors. STMicroelectronics’ products are also available from our online global e-commerce distributors, able to ship world-wide.. Digikey Electronics: www.digikey.com Farnell: www.farnell.com Mouser Electronics: www.mouser.com ST Online Support Center STMicroelectronics is a global independent semiconductor company and a leader in developing and delivering semiconductor solutions across the spectrum of microelectronics applications. An unrivaled combination of silicon and system expertise, manufacturing strength, Intellectual Property (IP) portfolio, and strategic partners positions, STMicroelectronics is at the forefront of System-on-Chip ... STMicroelectronics STMicroelectronics are available at Mouser Electronics. Mouser offers inventory, pricing, & datasheets for STMicroelectronics STMicroelectronics. Many IoT designs these days need location capabilities. While GPS alone was great in the old days, a modern design should be compatible with other satellite ... Mouser Electronics, Inc., the authorised global distributor with the newest semiconductors and electronic components, and STMicroelectronics (ST), a global semiconductor leader serving customers across the spectrum of electronics applications, have partnered to create a new resource site highlighting the products, insights and strategies required for smart industry designs. Recently, Mouser Electronics and celebrity engineer Grant Imahara unveiled the third video in the Engineering Big Ideas series, part of their award-winning Empowering Innovation Together program. stmicroelectronics Products are available at Mouser Electronics. Mouser offers inventory, pricing, & datasheets for stmicroelectronics Products. The major players covered in the medical electronics market report are Analog Devices, Texas Instruments Incorporated., Medtronic, STMicroelectronics, Mouser Electronics, Inc., Cypress Semiconductor Corporation, Semiconductor Components Industries, LLC, Digi-Key Electronics., Tekscan, Inc. and First Sensor AG, among other domestic and global ... An international Thyristor Discrete Semiconductor Market research report encompasses drivers and restraints for the market which are derived from the well-established SWOT analysis. The market report is a synopsis of the market facts, stats and figures for the forecast period of 2020 – 2027. Market definition, market segmentation, key developments in the market, competitive analysis and ... STMicroelectronics is a global independent semiconductor company and a leader in developing and delivering semiconductor solutions across the spectrum of microelectronics applications. An unrivaled combination of silicon and system expertise, manufacturing strength, Intellectual Property (IP) portfolio, and strategic partners positions, STMicroelectronics is at the forefront of System-on-Chip ... Transistors & Analog ICs. $3.99. Add to compare. Compare now. 38kHz Infrared (IR) Receiver Module. Model: 276-640 | Catalog #: 276-640. Great for experimental projects and building your own remote control. Mentor Graphics, STMicroelectronics and CEA-Leti join to launch the technological R&D programs of the NanoElec IRT. GRENOBLE, France – May 24, 2012. The ANR (French National Research Agency) and the CEA (French Atomic and Renewable Energy Commission) signed an agreement forming the Grenoble Institute of Technological Research (IRT): NanoElec ...\n------------------------\nProcessed corpus:\nstmicroelectron is a global independ semiconductor compani and a leader in develop and deliv semiconductor solut across the spectrum of microelectron applic an unriv combin of silicon and system expertis manufactur strength intellectu properti ip portfolio and strateg partner posit stmicroelectron is at the forefront of systemonchip stmicroelectron standard product are a broad rang of industrystandard and dropin replac for the most popular generalpurpos analog ic discret and serial eeprom the standard product are manufactur to the highest qualiti standard with mani aecqqualifi for automot applic mouser and mouser electron are cut tape product is cut from a full reel tape into custom quantiti mousereel add reel fee a product reel is cut accord to customerspecifi quantiti all mousereel order are noncancel and nonreturn full reel the order quantiti must match the manufactur full reel quantiti mousereel add reel fee a product reel is cut accord to customerspecifi quantiti all mousereel order are noncancel and nonreturn full reel the order quantiti must match the manufactur full reel quantiti to purchas full reel order in multipl of reel and cut tape stmfcbt stmicroelectron arm microcontrol mcu bit cortex m k med perform ln datasheet inventori price stmicroelectron is a global independ semiconductor compani and a leader in develop and deliv semiconductor solut across the spectrum of microelectron applic an unriv combin of silicon and system expertis manufactur strength intellectu properti ip portfolio and strateg partner posit stmicroelectron is at the forefront of systemonchip stmicroelectron is a lead integr devic manufactur deliv solut that are key to smart drive smart industri smart home citi and smart thing for quot and price in local currenc pleas select your countryregion to view our local st sale offic and distributor stmicroelectron product are also avail from our onlin global ecommerc distributor abl to ship worldwid digikey electron wwwdigikeycom farnel wwwfarnellcom mouser electron wwwmousercom st onlin support center stmicroelectron is a global independ semiconductor compani and a leader in develop and deliv semiconductor solut across the spectrum of microelectron applic an unriv combin of silicon and system expertis manufactur strength intellectu properti ip portfolio and strateg partner posit stmicroelectron is at the forefront of systemonchip stmicroelectron stmicroelectron are avail at mouser electron mouser offer inventori price datasheet for stmicroelectron stmicroelectron mani iot design these day need locat capabl while gp alon wa great in the old day a modern design should be compat with other satellit mouser electron inc the authoris global distributor with the newest semiconductor and electron compon and stmicroelectron st a global semiconductor leader serv custom across the spectrum of electron applic have partner to creat a new resourc site highlight the product insight and strategi requir for smart industri design recent mouser electron and celebr engin grant imahara unveil the third video in the engin big idea seri part of their awardwin empow innov togeth program stmicroelectron product are avail at mouser electron mouser offer inventori price datasheet for stmicroelectron product the major player cover in the medic electron market report are analog devic texa instrument incorpor medtron stmicroelectron mouser electron inc cypress semiconductor corpor semiconductor compon industri llc digikey electron tekscan inc and first sensor ag among other domest and global an intern thyristor discret semiconductor market research report encompass driver and restraint for the market which are deriv from the wellestablish swot analysi the market report is a synopsi of the market fact stat and figur for the forecast period of market definit market segment key develop in the market competit analysi and stmicroelectron is a global independ semiconductor compani and a leader in develop and deliv semiconductor solut across the spectrum of microelectron applic an unriv combin of silicon and system expertis manufactur strength intellectu properti ip portfolio and strateg partner posit stmicroelectron is at the forefront of systemonchip transistor analog ic add to compar compar now khz infrar ir receiv modul model catalog great for experiment project and build your own remot control mentor graphic stmicroelectron and cealeti join to launch the technolog rd program of the nanoelec irt grenobl franc may the anr french nation research agenc and the cea french atom and renew energi commiss sign an agreement form the grenobl institut of technolog research irt nanoelec \n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00006-a9b00a7b-d03d-49b6-bebf-250af7683d54",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ac6330c6",
    "execution_start": 1635959848760,
    "execution_millis": 275700,
    "deepnote_cell_type": "code"
   },
   "source": "# Creating new column for clean descirption\nfrom tqdm import tqdm\ndataset[\"description\"] = dataset[\"description\"].fillna(\".\")\ndataset[\"description\"] = dataset[\"description\"].astype(str)\nclean_descriptions=[]\ndescriptions=dataset[\"description\"].values \nfor desc in tqdm(descriptions):\n    clean_descriptions.append(preprocess(desc)) \ndataset['clean_description']=clean_descriptions",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "100%|██████████| 20000/20000 [04:35<00:00, 72.56it/s]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00007-e058ffda-6765-4065-a97a-e5ea5d8b012c",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a80ef6f9",
    "execution_start": 1635960177579,
    "execution_millis": 0,
    "deepnote_cell_type": "code"
   },
   "source": "\nsentences = dataset['clean_description'].values",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00009-dd5428d9-5390-4405-94bf-758510e60879",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "7fabecfb",
    "execution_start": 1635960184426,
    "execution_millis": 2,
    "deepnote_cell_type": "code"
   },
   "source": "dataset['clean_description']=dataset['clean_description'].values",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00009-b8401bf3-2d8d-43a9-b72b-ae606c2dcfb1",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "571c1c56",
    "execution_start": 1635960187600,
    "execution_millis": 4057,
    "deepnote_cell_type": "code"
   },
   "source": "#copying data with clean sets to new dataset\ndataset.to_csv('clean_set.csv', index=False)\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Cleaning industry data",
   "metadata": {
    "tags": [],
    "cell_id": "00011-60d4906b-f797-4f81-8ac4-942b289b7aa3",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00028-34c933ee-965c-4921-9074-243aae53207d",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6691663e",
    "execution_start": 1635960854508,
    "execution_millis": 1246,
    "deepnote_cell_type": "code"
   },
   "source": "training=pd.read_csv('industry_data.csv')\ntraining\n\ntraining[\"description\"] = training[\"description\"].fillna(\".\")\ntraining[\"description\"] = training[\"description\"].astype(str)\nclean_descriptions=[]\ndescriptions=training[\"description\"].values \nfor desc in tqdm(descriptions):\n    clean_descriptions.append(preprocess(desc)) \ntraining['clean_description']=clean_descriptions\ntraining['clean_description']=training['clean_description'].values.astype(str)\ntraining.to_csv('industry_data.csv', index=False)",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": "100%|██████████| 13/13 [00:01<00:00, 10.99it/s]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00007-dcb9f6f7-5b1d-432c-a0a7-c6d0a9dad41a",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "cb27a1c8",
    "execution_start": 1635961116928,
    "execution_millis": 9,
    "deepnote_cell_type": "code"
   },
   "source": "# Defining stopwords\nwith open(\"stopwords.txt\", \"r\") as f_in:\n    stopwords = [i.strip().lower() for i in f_in.readlines()]",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Count Vectorizer",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00008-f679cf84-4219-463a-8d27-dda50b30b4f8",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "markdown",
   "source": "An encoded vector is returned with a length of the entire vocabulary and an integer count for the number of times each word appeared in the document.",
   "metadata": {
    "tags": [],
    "cell_id": "00009-561a2278-0385-4dd7-a719-c68df16ea0d1",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00008-359f781a-2c0e-47a5-836b-22d1791ab4e3",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ad68294b",
    "execution_start": 1635961490943,
    "execution_millis": 2,
    "deepnote_cell_type": "code"
   },
   "source": "# Implementing Vectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Parameters that we can tune\nNGRAM = (1, 1) # Add more features when context is needed\nMIN_DF = 2 # Ignore terms that appear less than 10% of document\nMAX_DF = 0.4 # Ignore terms appear more than 30%\nMAX_FEATURES = 4000 # Define the lenght of the vocabulary\n\ncount_vec = CountVectorizer(\n    ngram_range = NGRAM,\n    tokenizer = lambda s: s.split(),\n    stop_words = stopwords,\n    min_df = MIN_DF,\n    max_df = MAX_DF,\n    max_features = MAX_FEATURES    \n)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00009-8adca74a-06cf-4ca9-978b-069b93887d71",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "76487de1",
    "execution_start": 1635957333849,
    "execution_millis": 37,
    "deepnote_cell_type": "code"
   },
   "source": "# Fit the vectorizer and explore data\nsample = dataset[\"clean_description\"].sample(100)\ncount_vec.fit(sample)\n\nprint(count_vec.vocabulary_)",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "{'resourc': 81, 'care': 12, 'network': 61, 'medic': 56, 'rang': 76, 'improv': 41, 'counti': 22, 'benefit': 7, 'healthcar': 37, 'patient': 67, 'research': 80, 'hospit': 39, 'facil': 31, 'integr': 44, 'career': 13, 'opportun': 65, 'share': 88, 'nation': 60, 'region': 78, 'educ': 27, 'visit': 97, 'senior': 87, 'join': 46, 'bank': 6, 'onlin': 64, 'account': 0, 'place': 69, 'hour': 40, 'insur': 43, 'institut': 42, 'rate': 77, 'specialist': 92, 'privat': 74, 'client': 16, 'agenc': 1, 'local': 52, 'regist': 79, 'call': 10, 'assist': 4, 'applic': 3, 'emerg': 28, 'complet': 18, 'onli': 63, 'life': 48, 'hi': 38, 'children': 14, 'depart': 25, 'live': 50, 'scienc': 85, 'llc': 51, 'memori': 58, 'salari': 82, 'power': 70, 'control': 21, 'announc': 2, 'manufactur': 54, 'softwar': 90, 'price': 73, 'well': 98, 'know': 47, 'secur': 86, 'sourc': 91, 'ga': 35, 'data': 23, 'capit': 11, 'director': 26, 'partner': 66, 'ltd': 53, 'uk': 96, 'train': 95, 'sale': 83, 'associ': 5, 'invest': 45, 'limit': 49, 'citi': 15, 'york': 99, 'global': 36, 'comput': 19, 'de': 24, 'site': 89, 'million': 59, 'stock': 93, 'firm': 33, 'execut': 30, 'consult': 20, 'food': 34, 'news': 62, 'perform': 68, 'build': 9, 'store': 94, 'school': 84, 'brand': 8, 'map': 55, 'present': 71, 'presid': 72, 'project': 75, 'energi': 29, 'file': 32, 'co': 17, 'medicin': 57}\n/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n  \"The parameter 'token_pattern' will not be used\"\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00012-8507dcf3-7f67-4c8d-845d-eedf92260796",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "784b535b",
    "execution_start": 1635957333883,
    "execution_millis": 39,
    "deepnote_cell_type": "code"
   },
   "source": "\nvector=count_vec.transform(sample)    \nvector.todense() #encoded sparse vectors to np arrays",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 11,
     "data": {
      "text/plain": "matrix([[0, 0, 0, ..., 1, 0, 0],\n        [7, 0, 0, ..., 0, 0, 0],\n        [1, 2, 0, ..., 1, 0, 0],\n        ...,\n        [0, 0, 1, ..., 0, 0, 2],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 2, 0, ..., 0, 0, 0]])"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Hashing Vectorizer",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00010-38d74199-bb85-4399-a9ad-322ae8845f4b",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Vocabularies can be large when using counts and frequencies, so hashing vectorizers does one way hash of words to convert them to integers, thus no vocabulary is needed and can choose an arbitrary-long fixed length vector. However, it cannot convert the encoding back to a word.",
   "metadata": {
    "tags": [],
    "cell_id": "00011-b1d1f172-8082-4fa4-bbef-132a1c38217b",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00010-c9a98c79-f175-497c-868f-2ab06e988890",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "95ed9e38",
    "execution_start": 1635957333930,
    "execution_millis": 40,
    "deepnote_cell_type": "code"
   },
   "source": "# Implementing Hashing Vectorizer\n\nfrom sklearn.feature_extraction.text import HashingVectorizer\n\nvectorizer = HashingVectorizer(n_features=20)\nvector = vectorizer.transform(sample)\nprint(vector.shape)\nprint(vector.toarray())",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "(100, 20)\n[[-0.05638839  0.09867968 -0.31013613 ... -0.09867968 -0.38062162\n  -0.0140971 ]\n [-0.04950738 -0.09901475  0.42081271 ...  0.07426107 -0.19802951\n   0.02475369]\n [-0.19014018  0.24718224  0.15211215 ...  0.26619626 -0.34225233\n   0.        ]\n ...\n [-0.17156089  0.          0.19062321 ... -0.11437393 -0.17156089\n   0.07624929]\n [ 0.16204746  0.13889782  0.34724455 ...  0.02314964  0.09259855\n  -0.11574818]\n [-0.3243575   0.01621787  0.08108937 ...  0.3243575  -0.27570387\n  -0.04865362]]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Training dataset with LDA",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00024-92a21a9b-291d-45b9-8ed5-ffb9772fb98b",
    "deepnote_cell_type": "text-cell-h3"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Tfidf/Term Frequency times Inverse Document Frequency/ are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents.",
   "metadata": {
    "tags": [],
    "cell_id": "00017-f56dde55-ca42-44e0-b4f5-b0df616c96bc",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00024-3ad19b6d-e5e9-4d61-9b06-165151d1b3d4",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c6b63534",
    "execution_start": 1636082477241,
    "execution_millis": 3132,
    "deepnote_cell_type": "code"
   },
   "source": "import pandas as pd\ntraining_df = pd.read_csv('industry_data.csv')\ntraining_corpus = training_df['clean_description'].values.astype(str)\nindustry_names= training_df['industry'].values\n\nemployer_df = pd.read_csv(\"clean_set.csv\")\nemployer_df = employer_df.drop(labels=[\"employers\", \"description\"], axis=1)\ncorpus = employer_df['clean_description'].values.astype(str)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00025-bb106703-349d-4681-adc5-77b5a05a9907",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "cb27a1c8",
    "execution_start": 1636082529343,
    "execution_millis": 1,
    "deepnote_cell_type": "code"
   },
   "source": "# Defining stopwords\nwith open(\"stopwords.txt\", \"r\") as f_in:\n    stopwords = [i.strip().lower() for i in f_in.readlines()]",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00018-82e54987-2418-479a-b661-4c92df9c40bd",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6825777e",
    "execution_start": 1636082533839,
    "execution_millis": 1394,
    "deepnote_cell_type": "code"
   },
   "source": "from sklearn.feature_extraction.text import TfidfVectorizer\nNGRAM = (1, 1) # Add more features when context is needed\nMIN_DF = 2 # Ignore terms that appear less than 10% of document\nMAX_DF = 0.4 # Ignore terms appear more than 30%\n#MAX_FEATURES = 4000 # Define the lenght of the vocabulary\n\nidf_vec = TfidfVectorizer(\n    #ngram_range=NGRAM,\n    tokenizer=lambda s: s.split(),\n    stop_words=stopwords,\n    min_df=MIN_DF,\n    max_df=MAX_DF,\n    #max_features=MAX_FEATURES,\n    use_idf=True,\n    smooth_idf=True\n)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00019-aaddecc6-1663-4073-8fc1-b59b5ab43181",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "83de62ac",
    "execution_start": 1636082537367,
    "execution_millis": 13,
    "deepnote_cell_type": "code"
   },
   "source": "#transforming industry data to vector by tfdif vectorizer\n\nvector_idf = idf_vec.fit_transform(training_corpus)\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00025-c46544ed-146a-4b4d-967c-520c83cdad99",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "2b04e356",
    "execution_start": 1636082539123,
    "execution_millis": 2839,
    "deepnote_cell_type": "code"
   },
   "source": "#importing lda \nfrom sklearn.decomposition import LatentDirichletAllocation as LDA\nlda = LDA(n_components=20,learning_method='online', learning_offset=40, n_jobs=-1)\n#getting industry topics\nindustry_topics=lda.fit_transform(vector_idf)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00023-38ff6981-730b-4794-83a5-6de6b68fcae3",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "18737f53",
    "execution_start": 1636082542869,
    "execution_millis": 7655,
    "deepnote_cell_type": "code"
   },
   "source": "#transforming employer data to vector by tfdif vectorizer\n#and creating topics by lda\nemployer_vectors=idf_vec.transform(corpus)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00030-61a756fd-4e12-4eae-862c-c44af8155322",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c0f54285",
    "execution_start": 1636082551394,
    "execution_millis": 11112,
    "deepnote_cell_type": "code"
   },
   "source": "employer_topics=lda.transform(employer_vectors)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00031-4d266a24-4048-4ba5-9d96-f409f5b9ce78",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "eb8cde34",
    "execution_start": 1636082563469,
    "execution_millis": 12,
    "deepnote_cell_type": "code"
   },
   "source": "print(len(employer_topics))",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "20000\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00026-6300dd81-f64f-4374-b294-788891e5e5a3",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "7a218a36",
    "execution_start": 1636082566857,
    "execution_millis": 3294,
    "deepnote_cell_type": "code"
   },
   "source": "import numpy as np\nindustry_prediction = []\nfor employer_vec in employer_topics:\n    distances = []\n    for industry_vec in industry_topics:\n        #Look at how close the company topics are from the industry\n        distances.append(np.linalg.norm(industry_vec - employer_vec))\n    #Pick the closest company\n    best_industry_index = np.argmin(distances)\n    industry_prediction.append(industry_names[best_industry_index])\n#print(len(industry_prediction))\n#components=lda.components_",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00032-0554a4ea-5560-48bf-acb0-e285bf30d9af",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "cb374486",
    "execution_start": 1636082572020,
    "execution_millis": 1,
    "deepnote_cell_type": "code"
   },
   "source": "\n\nemployer_df[\"lda_prediction\"] = industry_prediction",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00039-aa0bad4c-1d84-4df4-b454-7f994dc552f0",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "2ad54f55",
    "execution_start": 1636082574567,
    "execution_millis": 1740,
    "deepnote_cell_type": "code"
   },
   "source": "employer_df.to_csv('clean_set.csv', index=False)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00040-ef1a6b96-3c6f-40f2-945b-f5ef553c3c21",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### LDA Example",
   "metadata": {
    "tags": [],
    "cell_id": "00012-d76cce7c-c68c-4c4f-824b-4eb2e0666253",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00011-9ecffc28-544b-4bc7-9654-e059aacb95a0",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "e6c4acf",
    "execution_start": 1635906024087,
    "execution_millis": 130,
    "deepnote_cell_type": "code"
   },
   "source": "# Implementing Latent Dirichlet Allocation (LDA) model\n#It builds a topic per document model and words per topic model, modeled as Dirichlet distributions.\n\nimport gensim\nfrom gensim.test.utils import common_texts\nfrom gensim.corpora.dictionary import Dictionary\n\n# Create a corpus from a list of texts\ncommon_dictionary = Dictionary(common_texts)\ncommon_corpus = [common_dictionary.doc2bow(text) for text in common_texts]\n\nlda = gensim.models.LdaModel(common_corpus, num_topics=10)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00012-71e6229c-270e-428f-b2a7-2283dfe605e5",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "a45a3bf3",
    "execution_start": 1635906096281,
    "execution_millis": 2009,
    "deepnote_cell_type": "code"
   },
   "source": "import io\nimport os.path\nimport re\nimport tarfile\n\nimport smart_open\n\ndef extract_documents(url='https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'):\n    with smart_open.open(url, \"rb\") as file:\n        with tarfile.open(fileobj=file) as tar:\n            for member in tar.getmembers():\n                if member.isfile() and re.search(r'nipstxt/nips\\d+/\\d+\\.txt', member.name):\n                    member_bytes = tar.extractfile(member).read()\n                    yield member_bytes.decode('utf-8', errors='replace')\n\ndocs = list(extract_documents())",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00014-681d0c7a-9943-4efa-87d9-085dd9ce1036",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "ad350820",
    "execution_start": 1635906100464,
    "execution_millis": 4208,
    "deepnote_cell_type": "code"
   },
   "source": "# Tokenize the documents.\nfrom nltk.tokenize import RegexpTokenizer\n\n# Split the documents into tokens.\ntokenizer = RegexpTokenizer(r'\\w+')\nfor idx in range(len(docs)):\n    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n\n# Remove numbers, but not words that contain numbers.\ndocs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n\n# Remove words that are only one character.\ndocs = [[token for token in doc if len(token) > 1] for doc in docs]",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00015-57c1d14f-5e1a-4c5e-938c-7d33d8c5bc2f",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "c6d47f22",
    "execution_start": 1635906107224,
    "execution_millis": 34684,
    "deepnote_cell_type": "code"
   },
   "source": "# Lemmatize the documents.\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\ndocs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00016-10f9d6bc-dffe-4ed0-a73f-228ae6990608",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "c7a71441",
    "execution_start": 1635906159584,
    "execution_millis": 26087,
    "deepnote_cell_type": "code"
   },
   "source": "# Compute bigrams.\nfrom gensim.models import Phrases\n\n# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\nbigram = Phrases(docs, min_count=20)\nfor idx in range(len(docs)):\n    for token in bigram[docs[idx]]:\n        if '_' in token:\n            # Token is a bigram, add to document.\n            docs[idx].append(token)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00017-07cc0790-ad09-40cd-8cc8-05b246ac80d6",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "1f5a9bb1",
    "execution_start": 1635906187958,
    "execution_millis": 5855,
    "deepnote_cell_type": "code"
   },
   "source": "# Remove rare and common tokens.\nfrom gensim.corpora import Dictionary\n\n# Create a dictionary representation of the documents.\ndictionary = Dictionary(docs)\n\n# Filter out words that occur less than 20 documents, or more than 50% of the documents.\ndictionary.filter_extremes(no_below=20, no_above=0.5)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00018-9c768564-b99a-456c-96d9-09ba0662b26c",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "ba821fdc",
    "execution_start": 1635906198549,
    "execution_millis": 2961,
    "deepnote_cell_type": "code"
   },
   "source": "# Bag-of-words representation of the documents.\ncorpus = [dictionary.doc2bow(doc) for doc in docs]",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00019-4831df1e-5a9d-4b78-bb92-e8f653402789",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "f82c199d",
    "execution_start": 1635906203211,
    "execution_millis": 6,
    "deepnote_cell_type": "code"
   },
   "source": "print('Number of unique tokens: %d' % len(dictionary))\nprint('Number of documents: %d' % len(corpus))",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Number of unique tokens: 8644\nNumber of documents: 1740\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00020-b9175a2d-dee2-4e45-8acf-cd659874a23b",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "cb83c9c5",
    "execution_start": 1635906392938,
    "execution_millis": 134997,
    "deepnote_cell_type": "code"
   },
   "source": "# Train LDA model.\nfrom gensim.models import LdaModel\n\n# Set training parameters.\nnum_topics = 50\nchunksize = 2000\npasses = 20\n#iterations = 400\neval_every = None  # Don't evaluate model perplexity, takes too much time.\n\n# Make a index to word dictionary.\ntemp = dictionary[0]  # This is only to \"load\" the dictionary.\nid2word = dictionary.id2token\n\nmodel = LdaModel(\n    corpus=corpus,\n    id2word=id2word,\n    chunksize=chunksize,\n    alpha='auto',\n    eta='auto',\n    num_topics=num_topics,\n    passes=passes,\n    eval_every=eval_every\n)",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-0ac30220a36f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpasses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0meval_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_every\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m )\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m             self.add_lifecycle_event(\n\u001b[1;32m    522\u001b[0m                 \u001b[0;34m\"created\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                         \u001b[0mpass_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_no\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlencorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m                     )\n\u001b[0;32m-> 1005\u001b[0;31m                     \u001b[0mgammat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_estep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_alpha\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mdo_estep\u001b[0;34m(self, chunk, state)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollect_sstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msstats\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumdocs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# avoids calling len(chunk) on a generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[1;32m    708\u001b[0m             \u001b[0;31m# phinorm is the normalizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m             \u001b[0;31m# TODO treat zeros explicitly, instead of adding epsilon?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mphinorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpElogthetad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpElogbetad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m             \u001b[0;31m# Iterate between gamma and phi until convergence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00021-c239ad40-1bc0-4cf1-8474-336189992b13",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "64a17d1c",
    "execution_start": 1635365108374,
    "execution_millis": 359,
    "deepnote_cell_type": "code"
   },
   "source": "top_topics = model.top_topics(corpus) #, num_words=20)\n\navg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\nprint('Average topic coherence: %.4f.' % avg_topic_coherence)\n\nfrom pprint import pprint\npprint(top_topics)",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Average topic coherence: -1.1779.\n[([(0.007037914, 'gaussian'),\n   (0.0063822237, 'matrix'),\n   (0.0059578186, 'density'),\n   (0.0048813866, 'noise'),\n   (0.004702009, 'approximation'),\n   (0.004515913, 'prior'),\n   (0.00432641, 'bayesian'),\n   (0.0042887274, 'solution'),\n   (0.004117072, 'likelihood'),\n   (0.004024778, 'mixture'),\n   (0.0039797793, 'component'),\n   (0.0036772664, 'log'),\n   (0.003604146, 'estimate'),\n   (0.003441111, 'rule'),\n   (0.003437534, 'sample'),\n   (0.0034183024, 'variance'),\n   (0.0033686808, 'posterior'),\n   (0.0031950378, 'field'),\n   (0.0031777166, 'xi'),\n   (0.0029016682, 'optimal')],\n  -0.9087692348419095),\n ([(0.020912563, 'neuron'),\n   (0.017845048, 'cell'),\n   (0.00756838, 'spike'),\n   (0.007193502, 'response'),\n   (0.006995943, 'synaptic'),\n   (0.0067574014, 'activity'),\n   (0.005945886, 'stimulus'),\n   (0.0058641955, 'firing'),\n   (0.0047210045, 'connection'),\n   (0.0045576114, 'cortex'),\n   (0.0043599596, 'field'),\n   (0.0042713624, 'visual'),\n   (0.0041241385, 'map'),\n   (0.003924601, 'cortical'),\n   (0.0038658425, 'fig'),\n   (0.0037249718, 'layer'),\n   (0.0037078538, 'simulation'),\n   (0.0036443798, 'potential'),\n   (0.0035859565, 'eye'),\n   (0.003369004, 'synapsis')],\n  -0.9798016287664303),\n ([(0.005786256, 'generalization'),\n   (0.0057753236, 'classifier'),\n   (0.0055298293, 'class'),\n   (0.0047362917, 'training_set'),\n   (0.004706815, 'prediction'),\n   (0.0043786564, 'regression'),\n   (0.004216339, 'kernel'),\n   (0.003805116, 'classification'),\n   (0.0037435405, 'sample'),\n   (0.0035283288, 'decision'),\n   (0.0034380907, 'gradient'),\n   (0.0033496742, 'tree'),\n   (0.0032547999, 'validation'),\n   (0.003253471, 'estimate'),\n   (0.0030431729, 'hidden'),\n   (0.003029639, 'loss'),\n   (0.0030141142, 'machine'),\n   (0.002958731, 'noise'),\n   (0.0027449995, 'optimal'),\n   (0.0026439284, 'support')],\n  -0.9931443585444112),\n ([(0.009037506, 'image'),\n   (0.008414052, 'visual'),\n   (0.00789665, 'motion'),\n   (0.00762542, 'signal'),\n   (0.0057515567, 'memory'),\n   (0.0057473592, 'filter'),\n   (0.005734494, 'noise'),\n   (0.005571634, 'response'),\n   (0.0054459693, 'field'),\n   (0.0053177685, 'neuron'),\n   (0.0049232207, 'stimulus'),\n   (0.0049208635, 'component'),\n   (0.004290656, 'frequency'),\n   (0.0042161606, 'direction'),\n   (0.004212759, 'spatial'),\n   (0.0041142753, 'dynamic'),\n   (0.0039084083, 'phase'),\n   (0.0032162424, 'cell'),\n   (0.0032127034, 'attractor'),\n   (0.0030838014, 'temporal')],\n  -1.0750249733115143),\n ([(0.011585814, 'hidden'),\n   (0.009093658, 'node'),\n   (0.0083712125, 'net'),\n   (0.007494472, 'layer'),\n   (0.006981665, 'hidden_unit'),\n   (0.00474069, 'activation'),\n   (0.0046693445, 'threshold'),\n   (0.004666516, 'bound'),\n   (0.004230367, 'tree'),\n   (0.004094967, 'theorem'),\n   (0.0040248353, 'sequence'),\n   (0.0037617534, 'rule'),\n   (0.003668778, 'let'),\n   (0.0031585235, 'dimension'),\n   (0.0030838305, 'class'),\n   (0.0030673956, 'language'),\n   (0.0029949665, 'connection'),\n   (0.002980696, 'recurrent'),\n   (0.0029041404, 'proof'),\n   (0.0028868543, 'architecture')],\n  -1.1793433850684754),\n ([(0.013828947, 'control'),\n   (0.013581753, 'action'),\n   (0.010261094, 'policy'),\n   (0.008863972, 'reinforcement'),\n   (0.0074952897, 'optimal'),\n   (0.0066883652, 'controller'),\n   (0.006122893, 'dynamic'),\n   (0.0058623813, 'reinforcement_learning'),\n   (0.005540643, 'robot'),\n   (0.005004726, 'environment'),\n   (0.004750506, 'reward'),\n   (0.0046573794, 'trajectory'),\n   (0.004302987, 'goal'),\n   (0.003750879, 'decision'),\n   (0.0037304917, 'path'),\n   (0.0035306516, 'sutton'),\n   (0.0033021069, 'td'),\n   (0.0032728356, 'agent'),\n   (0.0032259047, 'cost'),\n   (0.003131487, 'trial')],\n  -1.2604701640366545),\n ([(0.013569687, 'speech'),\n   (0.010551207, 'recognition'),\n   (0.008080591, 'word'),\n   (0.0068083, 'sequence'),\n   (0.006419012, 'mixture'),\n   (0.0063444925, 'hidden'),\n   (0.006172426, 'context'),\n   (0.0057078796, 'expert'),\n   (0.005630329, 'hmm'),\n   (0.0052600033, 'speaker'),\n   (0.0052401503, 'architecture'),\n   (0.0047840094, 'signal'),\n   (0.0046429704, 'layer'),\n   (0.0041974545, 'recurrent'),\n   (0.0040209005, 'markov'),\n   (0.0038477941, 'trained'),\n   (0.0037808889, 'likelihood'),\n   (0.0037093202, 'node'),\n   (0.0035075552, 'frame'),\n   (0.003439392, 'phoneme')],\n  -1.2727256550618053),\n ([(0.0061427034, 'rule'),\n   (0.0059446073, 'layer'),\n   (0.0055249077, 'net'),\n   (0.005008931, 'hidden'),\n   (0.0049339463, 'trained'),\n   (0.0044194367, 'classifier'),\n   (0.0038431752, 'human'),\n   (0.0036607983, 'table'),\n   (0.0036202334, 'position'),\n   (0.003537209, 'memory'),\n   (0.0033690985, 'classification'),\n   (0.0033270575, 'architecture'),\n   (0.0032974728, 'hand'),\n   (0.0032274702, 'user'),\n   (0.0029975935, 'block'),\n   (0.0029331418, 'signal'),\n   (0.0028717848, 'detection'),\n   (0.002670744, 'processor'),\n   (0.0026615933, 'parallel'),\n   (0.0026601981, 'bit')],\n  -1.3111292761798645),\n ([(0.013503865, 'circuit'),\n   (0.010785265, 'chip'),\n   (0.009672104, 'analog'),\n   (0.0069868327, 'word'),\n   (0.0067199725, 'voltage'),\n   (0.006549893, 'character'),\n   (0.005573344, 'image'),\n   (0.0055202255, 'signal'),\n   (0.005123491, 'vlsi'),\n   (0.0047997003, 'recognition'),\n   (0.004374643, 'channel'),\n   (0.0039989916, 'frequency'),\n   (0.0037219417, 'implementation'),\n   (0.0036922568, 'sound'),\n   (0.0035842021, 'field'),\n   (0.0033015313, 'pixel'),\n   (0.0032405602, 'node'),\n   (0.0032322276, 'transistor'),\n   (0.003104341, 'layer'),\n   (0.0030417845, 'pulse')],\n  -1.387118004745816),\n ([(0.021654015, 'image'),\n   (0.013368483, 'object'),\n   (0.009712458, 'distance'),\n   (0.008690487, 'recognition'),\n   (0.0076657715, 'class'),\n   (0.0066732066, 'face'),\n   (0.005723974, 'classification'),\n   (0.0055819475, 'cluster'),\n   (0.0050485423, 'view'),\n   (0.004773794, 'transformation'),\n   (0.004328438, 'dimensional'),\n   (0.0038727208, 'pixel'),\n   (0.0033407116, 'tangent'),\n   (0.0032806925, 'clustering'),\n   (0.0031538478, 'neighbor'),\n   (0.0030490353, 'nearest'),\n   (0.003005038, 'rotation'),\n   (0.002794679, 'region'),\n   (0.0026341178, 'measure'),\n   (0.0026334433, 'matching')],\n  -1.4110643961322349)]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00022-e266873f-36e6-45a8-ab35-e127e454f72b",
    "deepnote_to_be_reexecuted": true,
    "source_hash": "b623e53d",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=fb3de8ef-fb47-4eee-bfce-0a5c05122f97' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "95ea3ea6-ed70-426e-98b7-f36e1fac8f5b",
  "deepnote_execution_queue": []
 }
}